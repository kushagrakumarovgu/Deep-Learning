{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OfvGHa9d5Xg"
      },
      "source": [
        "Assignment 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JxOyFa0HNDXx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from datasets import MNISTDataset\n",
        "from time import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RdQEYgozNj0R",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# get the data\n",
        "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n",
        "mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n",
        "                     test_imgs.reshape((-1, 784)), test_lbls,\n",
        "                     batch_size=256, seed=int(time()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fEVxI4hINzLf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#set up log dir and file writer(s)\n",
        "logdir = os.path.join(\"logs\", \"linear\" + str(time()))\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))\n",
        "test_writer = tf.summary.create_file_writer(os.path.join(logdir, \"test\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gZ5Hx4enNtQS",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# define the model first, from input to output\n",
        "\n",
        "# let's use fewer layers...\n",
        "n_units = 100\n",
        "n_layers = 8 #8 #2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXSVILtgN3mK",
        "outputId": "4a5e2c45-030e-4fd3-c8ee-6cad64e6c933",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 2.3025853633880615 Accuracy: 0.0859375\n",
            "Loss: 2.3032422065734863 Accuracy: 0.1015625\n",
            "Loss: 2.3012046813964844 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2978227138519287 Accuracy: 0.125\n",
            "Loss: 2.3035643100738525 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.300755023956299 Accuracy: 0.125\n",
            "Loss: 2.2992873191833496 Accuracy: 0.125\n",
            "Loss: 2.2991855144500732 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.2971301078796387 Accuracy: 0.13671875\n",
            "Loss: 2.298671245574951 Accuracy: 0.10546875\n",
            "Starting new epoch...\n",
            "Loss: 2.300069808959961 Accuracy: 0.12109375\n",
            "Loss: 2.3011765480041504 Accuracy: 0.10546875\n",
            "Starting new epoch...\n",
            "Loss: 2.3023598194122314 Accuracy: 0.10546875\n",
            "Loss: 2.2998876571655273 Accuracy: 0.12890625\n",
            "Loss: 2.300877571105957 Accuracy: 0.12109375\n",
            "Starting new epoch...\n",
            "Loss: 2.3037524223327637 Accuracy: 0.1015625\n",
            "Loss: 2.299329996109009 Accuracy: 0.11328125\n",
            "Starting new epoch...\n",
            "Loss: 2.30256986618042 Accuracy: 0.09765625\n",
            "Loss: 2.306338310241699 Accuracy: 0.10546875\n",
            "Starting new epoch...\n",
            "Loss: 2.299285888671875 Accuracy: 0.125\n"
          ]
        }
      ],
      "source": [
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the output layer\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01)))\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "\n",
        "    # I hear adding random noise to inputs helps with generalization! \n",
        "    #commented below code to test the accuracy.\n",
        "    img_batch = img_batch + tf.random.normal(tf.shape(img_batch), stddev=0)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        with train_writer.as_default():\n",
        "          tf.summary.scalar(\"accuracy\", acc, step=step)\n",
        "          tf.summary.scalar(\"loss\", xent, step=step)\n",
        "          tf.summary.image(\"input\", tf.reshape(img_batch, [-1, 28, 28, 1]), step=step)\n",
        "          tf.summary.histogram(\"logits\", out, step=step)\n",
        "          tf.summary.histogram(\"weights\", var, step=step)\n",
        "          for var in weights:\n",
        "            tf.summary.histogram(var.name,var,step=step)\n",
        "          for i in range(0,len(weights)):\n",
        "            tf.summary.histogram(\"Gradient-\"+str(i),grads[i],step=step)\n",
        "    \n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ1wqKELON1R",
        "outputId": "97130d6b-bac4-4b2f-fcce-d3a5ad49e8b7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final test accuracy: 0.11349999904632568\n"
          ]
        }
      ],
      "source": [
        "out = mnist.test_data\n",
        "for layer in layers:\n",
        "    out = layer(out)\n",
        "test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "print(\"Final test accuracy: {}\".format(acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vFpu9rBMOZPQ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# then load/run tensorboard\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "id": "wkp2hbxYOZ7h",
        "outputId": "91e5ce24-26cb-48fe-9ecf-1018ac36ede5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 149), started 1:15:04 ago. (Use '!kill 149' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "%tensorboard --logdir logs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Script-4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
