{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae5xFWq_KNbF"
      },
      "source": [
        "Assignment 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l0ow4CqpCryI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpDfQK7sCuEW",
        "outputId": "746ad856-cbe3-4a85-d98a-4c0e80673727",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-12-07 07:49:26.903750: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Split input into 31103 sequences...\n",
            "Longest sequence is 533 characters. If this seems unreasonable, consider using the maxlen argument!\n",
            "Removing sequences longer than 500 characters...\n",
            "31102 sequences remaining.\n",
            "Longest remaining sequence has length 448.\n",
            "Removing length-0 sequences...\n",
            "31102 sequences remaining.\n",
            "Serialized 100 sequences...\n",
            "Serialized 200 sequences...\n",
            "Serialized 300 sequences...\n",
            "Serialized 400 sequences...\n",
            "Serialized 500 sequences...\n",
            "Serialized 600 sequences...\n",
            "Serialized 700 sequences...\n",
            "Serialized 800 sequences...\n",
            "Serialized 900 sequences...\n",
            "Serialized 1000 sequences...\n",
            "Serialized 1100 sequences...\n",
            "Serialized 1200 sequences...\n",
            "Serialized 1300 sequences...\n",
            "Serialized 1400 sequences...\n",
            "Serialized 1500 sequences...\n",
            "Serialized 1600 sequences...\n",
            "Serialized 1700 sequences...\n",
            "Serialized 1800 sequences...\n",
            "Serialized 1900 sequences...\n",
            "Serialized 2000 sequences...\n",
            "Serialized 2100 sequences...\n",
            "Serialized 2200 sequences...\n",
            "Serialized 2300 sequences...\n",
            "Serialized 2400 sequences...\n",
            "Serialized 2500 sequences...\n",
            "Serialized 2600 sequences...\n",
            "Serialized 2700 sequences...\n",
            "Serialized 2800 sequences...\n",
            "Serialized 2900 sequences...\n",
            "Serialized 3000 sequences...\n",
            "Serialized 3100 sequences...\n",
            "Serialized 3200 sequences...\n",
            "Serialized 3300 sequences...\n",
            "Serialized 3400 sequences...\n",
            "Serialized 3500 sequences...\n",
            "Serialized 3600 sequences...\n",
            "Serialized 3700 sequences...\n",
            "Serialized 3800 sequences...\n",
            "Serialized 3900 sequences...\n",
            "Serialized 4000 sequences...\n",
            "Serialized 4100 sequences...\n",
            "Serialized 4200 sequences...\n",
            "Serialized 4300 sequences...\n",
            "Serialized 4400 sequences...\n",
            "Serialized 4500 sequences...\n",
            "Serialized 4600 sequences...\n",
            "Serialized 4700 sequences...\n",
            "Serialized 4800 sequences...\n",
            "Serialized 4900 sequences...\n",
            "Serialized 5000 sequences...\n",
            "Serialized 5100 sequences...\n",
            "Serialized 5200 sequences...\n",
            "Serialized 5300 sequences...\n",
            "Serialized 5400 sequences...\n",
            "Serialized 5500 sequences...\n",
            "Serialized 5600 sequences...\n",
            "Serialized 5700 sequences...\n",
            "Serialized 5800 sequences...\n",
            "Serialized 5900 sequences...\n",
            "Serialized 6000 sequences...\n",
            "Serialized 6100 sequences...\n",
            "Serialized 6200 sequences...\n",
            "Serialized 6300 sequences...\n",
            "Serialized 6400 sequences...\n",
            "Serialized 6500 sequences...\n",
            "Serialized 6600 sequences...\n",
            "Serialized 6700 sequences...\n",
            "Serialized 6800 sequences...\n",
            "Serialized 6900 sequences...\n",
            "Serialized 7000 sequences...\n",
            "Serialized 7100 sequences...\n",
            "Serialized 7200 sequences...\n",
            "Serialized 7300 sequences...\n",
            "Serialized 7400 sequences...\n",
            "Serialized 7500 sequences...\n",
            "Serialized 7600 sequences...\n",
            "Serialized 7700 sequences...\n",
            "Serialized 7800 sequences...\n",
            "Serialized 7900 sequences...\n",
            "Serialized 8000 sequences...\n",
            "Serialized 8100 sequences...\n",
            "Serialized 8200 sequences...\n",
            "Serialized 8300 sequences...\n",
            "Serialized 8400 sequences...\n",
            "Serialized 8500 sequences...\n",
            "Serialized 8600 sequences...\n",
            "Serialized 8700 sequences...\n",
            "Serialized 8800 sequences...\n",
            "Serialized 8900 sequences...\n",
            "Serialized 9000 sequences...\n",
            "Serialized 9100 sequences...\n",
            "Serialized 9200 sequences...\n",
            "Serialized 9300 sequences...\n",
            "Serialized 9400 sequences...\n",
            "Serialized 9500 sequences...\n",
            "Serialized 9600 sequences...\n",
            "Serialized 9700 sequences...\n",
            "Serialized 9800 sequences...\n",
            "Serialized 9900 sequences...\n",
            "Serialized 10000 sequences...\n",
            "Serialized 10100 sequences...\n",
            "Serialized 10200 sequences...\n",
            "Serialized 10300 sequences...\n",
            "Serialized 10400 sequences...\n",
            "Serialized 10500 sequences...\n",
            "Serialized 10600 sequences...\n",
            "Serialized 10700 sequences...\n",
            "Serialized 10800 sequences...\n",
            "Serialized 10900 sequences...\n",
            "Serialized 11000 sequences...\n",
            "Serialized 11100 sequences...\n",
            "Serialized 11200 sequences...\n",
            "Serialized 11300 sequences...\n",
            "Serialized 11400 sequences...\n",
            "Serialized 11500 sequences...\n",
            "Serialized 11600 sequences...\n",
            "Serialized 11700 sequences...\n",
            "Serialized 11800 sequences...\n",
            "Serialized 11900 sequences...\n",
            "Serialized 12000 sequences...\n",
            "Serialized 12100 sequences...\n",
            "Serialized 12200 sequences...\n",
            "Serialized 12300 sequences...\n",
            "Serialized 12400 sequences...\n",
            "Serialized 12500 sequences...\n",
            "Serialized 12600 sequences...\n",
            "Serialized 12700 sequences...\n",
            "Serialized 12800 sequences...\n",
            "Serialized 12900 sequences...\n",
            "Serialized 13000 sequences...\n",
            "Serialized 13100 sequences...\n",
            "Serialized 13200 sequences...\n",
            "Serialized 13300 sequences...\n",
            "Serialized 13400 sequences...\n",
            "Serialized 13500 sequences...\n",
            "Serialized 13600 sequences...\n",
            "Serialized 13700 sequences...\n",
            "Serialized 13800 sequences...\n",
            "Serialized 13900 sequences...\n",
            "Serialized 14000 sequences...\n",
            "Serialized 14100 sequences...\n",
            "Serialized 14200 sequences...\n",
            "Serialized 14300 sequences...\n",
            "Serialized 14400 sequences...\n",
            "Serialized 14500 sequences...\n",
            "Serialized 14600 sequences...\n",
            "Serialized 14700 sequences...\n",
            "Serialized 14800 sequences...\n",
            "Serialized 14900 sequences...\n",
            "Serialized 15000 sequences...\n",
            "Serialized 15100 sequences...\n",
            "Serialized 15200 sequences...\n",
            "Serialized 15300 sequences...\n",
            "Serialized 15400 sequences...\n",
            "Serialized 15500 sequences...\n",
            "Serialized 15600 sequences...\n",
            "Serialized 15700 sequences...\n",
            "Serialized 15800 sequences...\n",
            "Serialized 15900 sequences...\n",
            "Serialized 16000 sequences...\n",
            "Serialized 16100 sequences...\n",
            "Serialized 16200 sequences...\n",
            "Serialized 16300 sequences...\n",
            "Serialized 16400 sequences...\n",
            "Serialized 16500 sequences...\n",
            "Serialized 16600 sequences...\n",
            "Serialized 16700 sequences...\n",
            "Serialized 16800 sequences...\n",
            "Serialized 16900 sequences...\n",
            "Serialized 17000 sequences...\n",
            "Serialized 17100 sequences...\n",
            "Serialized 17200 sequences...\n",
            "Serialized 17300 sequences...\n",
            "Serialized 17400 sequences...\n",
            "Serialized 17500 sequences...\n",
            "Serialized 17600 sequences...\n",
            "Serialized 17700 sequences...\n",
            "Serialized 17800 sequences...\n",
            "Serialized 17900 sequences...\n",
            "Serialized 18000 sequences...\n",
            "Serialized 18100 sequences...\n",
            "Serialized 18200 sequences...\n",
            "Serialized 18300 sequences...\n",
            "Serialized 18400 sequences...\n",
            "Serialized 18500 sequences...\n",
            "Serialized 18600 sequences...\n",
            "Serialized 18700 sequences...\n",
            "Serialized 18800 sequences...\n",
            "Serialized 18900 sequences...\n",
            "Serialized 19000 sequences...\n",
            "Serialized 19100 sequences...\n",
            "Serialized 19200 sequences...\n",
            "Serialized 19300 sequences...\n",
            "Serialized 19400 sequences...\n",
            "Serialized 19500 sequences...\n",
            "Serialized 19600 sequences...\n",
            "Serialized 19700 sequences...\n",
            "Serialized 19800 sequences...\n",
            "Serialized 19900 sequences...\n",
            "Serialized 20000 sequences...\n",
            "Serialized 20100 sequences...\n",
            "Serialized 20200 sequences...\n",
            "Serialized 20300 sequences...\n",
            "Serialized 20400 sequences...\n",
            "Serialized 20500 sequences...\n",
            "Serialized 20600 sequences...\n",
            "Serialized 20700 sequences...\n",
            "Serialized 20800 sequences...\n",
            "Serialized 20900 sequences...\n",
            "Serialized 21000 sequences...\n",
            "Serialized 21100 sequences...\n",
            "Serialized 21200 sequences...\n",
            "Serialized 21300 sequences...\n",
            "Serialized 21400 sequences...\n",
            "Serialized 21500 sequences...\n",
            "Serialized 21600 sequences...\n",
            "Serialized 21700 sequences...\n",
            "Serialized 21800 sequences...\n",
            "Serialized 21900 sequences...\n",
            "Serialized 22000 sequences...\n",
            "Serialized 22100 sequences...\n",
            "Serialized 22200 sequences...\n",
            "Serialized 22300 sequences...\n",
            "Serialized 22400 sequences...\n",
            "Serialized 22500 sequences...\n",
            "Serialized 22600 sequences...\n",
            "Serialized 22700 sequences...\n",
            "Serialized 22800 sequences...\n",
            "Serialized 22900 sequences...\n",
            "Serialized 23000 sequences...\n",
            "Serialized 23100 sequences...\n",
            "Serialized 23200 sequences...\n",
            "Serialized 23300 sequences...\n",
            "Serialized 23400 sequences...\n",
            "Serialized 23500 sequences...\n",
            "Serialized 23600 sequences...\n",
            "Serialized 23700 sequences...\n",
            "Serialized 23800 sequences...\n",
            "Serialized 23900 sequences...\n",
            "Serialized 24000 sequences...\n",
            "Serialized 24100 sequences...\n",
            "Serialized 24200 sequences...\n",
            "Serialized 24300 sequences...\n",
            "Serialized 24400 sequences...\n",
            "Serialized 24500 sequences...\n",
            "Serialized 24600 sequences...\n",
            "Serialized 24700 sequences...\n",
            "Serialized 24800 sequences...\n",
            "Serialized 24900 sequences...\n",
            "Serialized 25000 sequences...\n",
            "Serialized 25100 sequences...\n",
            "Serialized 25200 sequences...\n",
            "Serialized 25300 sequences...\n",
            "Serialized 25400 sequences...\n",
            "Serialized 25500 sequences...\n",
            "Serialized 25600 sequences...\n",
            "Serialized 25700 sequences...\n",
            "Serialized 25800 sequences...\n",
            "Serialized 25900 sequences...\n",
            "Serialized 26000 sequences...\n",
            "Serialized 26100 sequences...\n",
            "Serialized 26200 sequences...\n",
            "Serialized 26300 sequences...\n",
            "Serialized 26400 sequences...\n",
            "Serialized 26500 sequences...\n",
            "Serialized 26600 sequences...\n",
            "Serialized 26700 sequences...\n",
            "Serialized 26800 sequences...\n",
            "Serialized 26900 sequences...\n",
            "Serialized 27000 sequences...\n",
            "Serialized 27100 sequences...\n",
            "Serialized 27200 sequences...\n",
            "Serialized 27300 sequences...\n",
            "Serialized 27400 sequences...\n",
            "Serialized 27500 sequences...\n",
            "Serialized 27600 sequences...\n",
            "Serialized 27700 sequences...\n",
            "Serialized 27800 sequences...\n",
            "Serialized 27900 sequences...\n",
            "Serialized 28000 sequences...\n",
            "Serialized 28100 sequences...\n",
            "Serialized 28200 sequences...\n",
            "Serialized 28300 sequences...\n",
            "Serialized 28400 sequences...\n",
            "Serialized 28500 sequences...\n",
            "Serialized 28600 sequences...\n",
            "Serialized 28700 sequences...\n",
            "Serialized 28800 sequences...\n",
            "Serialized 28900 sequences...\n",
            "Serialized 29000 sequences...\n",
            "Serialized 29100 sequences...\n",
            "Serialized 29200 sequences...\n",
            "Serialized 29300 sequences...\n",
            "Serialized 29400 sequences...\n",
            "Serialized 29500 sequences...\n",
            "Serialized 29600 sequences...\n",
            "Serialized 29700 sequences...\n",
            "Serialized 29800 sequences...\n",
            "Serialized 29900 sequences...\n",
            "Serialized 30000 sequences...\n",
            "Serialized 30100 sequences...\n",
            "Serialized 30200 sequences...\n",
            "Serialized 30300 sequences...\n",
            "Serialized 30400 sequences...\n",
            "Serialized 30500 sequences...\n",
            "Serialized 30600 sequences...\n",
            "Serialized 30700 sequences...\n",
            "Serialized 30800 sequences...\n",
            "Serialized 30900 sequences...\n",
            "Serialized 31000 sequences...\n",
            "Serialized 31100 sequences...\n"
          ]
        }
      ],
      "source": [
        "!python3 prepare_data2.py the-king-james-bible.txt bible [0-9]+:[0-9]+ -m 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpnCZwRwKJzU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KTo_Z5KiTNw",
        "outputId": "26f9e267-eb17-4cdf-c48f-8680547d3c5c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'P': 3, 'N': 4, 'd': 5, 'H': 6, 'B': 7, '8': 8, 'E': 9, '\\ufeff': 10, 'G': 11, 'S': 12, '5': 13, '0': 14, '2': 15, ')': 16, ';': 17, 'R': 18, '!': 19, '*': 20, 'g': 21, 'U': 22, '\\n': 23, 'm': 24, 'F': 25, 't': 26, 'D': 27, 'r': 28, 'T': 29, 'p': 30, '7': 31, 'x': 32, 'j': 33, 'V': 34, 'L': 35, '-': 36, 'K': 37, 'a': 38, 'A': 39, '6': 40, 'k': 41, 'w': 42, '9': 43, 'i': 44, 'q': 45, '(': 46, 'O': 47, '.': 48, 'e': 49, 'u': 50, 'h': 51, '1': 52, 'I': 53, '3': 54, 'y': 55, '?': 56, 'J': 57, 'n': 58, 'f': 59, 'c': 60, 'z': 61, 'M': 62, 'l': 63, '4': 64, 'v': 65, 'Y': 66, 'Q': 67, \"'\": 68, 'o': 69, 'W': 70, 'Z': 71, ' ': 72, ',': 73, 'b': 74, 'C': 75, 's': 76, ':': 77, '<PAD>': 0, '<S>': 1, '</S>': 2}\n",
            "78\n"
          ]
        }
      ],
      "source": [
        "from prepare_data2 import parse_seq\n",
        "import pickle\n",
        "\n",
        "# this is just a datasets of \"bytes\" (not understandable)\n",
        "data = tf.data.TFRecordDataset(\"bible.tfrecords\")\n",
        "\n",
        "# this maps a parser function that properly interprets the bytes over the dataset\n",
        "# (with fixed sequence length 200)\n",
        "# if you change the sequence length in preprocessing you also need to change it here\n",
        "data = data.map(lambda x: parse_seq(x))\n",
        "\n",
        "# a map from characters to indices\n",
        "vocab = pickle.load(open(\"bible_vocab\", mode=\"rb\"))\n",
        "vocab_size = len(vocab)\n",
        "# inverse mapping: indices to characters\n",
        "ind_to_ch = {ind: ch for (ch, ind) in vocab.items()}\n",
        "#ch_to_ind = {ch: ind for (ch, ind) in vocab.items()}\n",
        "#print(np.array(data).shape)\n",
        "\n",
        "print(vocab)\n",
        "print(vocab_size)\n",
        "#print([ind_to_ch[3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MiN6rAeeO1yX",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#SimpleRNN\n",
        "def get_model_SimpleRNN(num_units,vocab_size):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.SimpleRNN(units=num_units,activation='tanh',stateful=True,return_sequences=True),\n",
        "                               tf.keras.layers.Dense(vocab_size)])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# GRU\n",
        "def get_model_GRU(num_units,vocab_size):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.GRU(units=num_units,stateful=True,return_sequences=True),\n",
        "                               tf.keras.layers.Dense(vocab_size)])\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "#LSTM\n",
        "def get_model_LSTM(num_units,vocab_size):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.LSTM(units=num_units,stateful=True,return_sequences=True),\n",
        "                               tf.keras.layers.Dense(vocab_size)])\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def cal_actual_loss(batch,xent):\n",
        "  non_zero_count = tf.math.count_nonzero(batch, axis=-1) - 1\n",
        "  mask = tf.cast(tf.sequence_mask(non_zero_count), tf.float32)\n",
        "  masked_loss = mask * xent\n",
        "  masked_loss_reduced = tf.reduce_sum(masked_loss)\n",
        "  non_zero_count_reduced =tf.reduce_sum(non_zero_count) #tf.dtypes.cast(tf.reduce_sum(non_zero_count), tf.float32)\n",
        "  total_loss = masked_loss_reduced / float(non_zero_count_reduced)\n",
        "  return total_loss\n",
        "\n",
        "\n",
        "#@tf.function\n",
        "def train_models(model,epochs,train_data):\n",
        "  for epoch in range(epochs):\n",
        "    print(\"Epoch: {}\".format(epoch))\n",
        "    for step, batch in enumerate(train_data):\n",
        "      oh_batch = tf.one_hot(batch,depth=vocab_size)\n",
        "      out_char = oh_batch[:,:-1]\n",
        "      with tf.GradientTape() as tape:\n",
        "        logits = model(oh_batch)\n",
        "        xent = tf.nn.softmax_cross_entropy_with_logits(out_char,logits[:,1:])\n",
        "        actual_loss = cal_actual_loss(batch,xent)\n",
        "      grads = tape.gradient(actual_loss,model.trainable_variables)\n",
        "      optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
        "      \n",
        "      if not step % 100:\n",
        "        print(\"Loss: {} \".format(actual_loss))\n",
        "    model.reset_states()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZimjDHk67eZ6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "list_ind = range(0,78)\n",
        "def gen_sequence():\n",
        "  sequence = \"\"\n",
        "  count = 0\n",
        "  batch_size = 128\n",
        "  x = tf.one_hot(indices = 1, depth = vocab_size)\n",
        "  input = np.zeros(shape=[128,1,78],dtype=float)\n",
        "  for idx in range(0,128):\n",
        "    input[idx] = x\n",
        "  input_char = tf.convert_to_tensor(input,dtype=tf.float32) \n",
        "  while count < 5000:\n",
        "    output = model(input_char)\n",
        "    y_pred = tf.nn.softmax(output)\n",
        "    input_char = output   \n",
        "    for idx in range(batch_size):\n",
        "      y_pred_numpy = y_pred[idx].numpy()\n",
        "      indx = np.random.choice( list_ind, p = y_pred_numpy.flatten())\n",
        "      if ( ind_to_ch[indx] != \"</S>\"):\n",
        "        sequence += ind_to_ch[indx]    \n",
        "      count += 1\n",
        "    model.reset_states() \n",
        "    \n",
        "  return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0GABNUVZhLs",
        "outputId": "c00231ea-aa3c-4cca-d07f-157bcd394885",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Loss: 4.364647388458252 \n",
            "Loss: 2.8957431316375732 \n",
            "Loss: 2.6242353916168213 \n",
            "Epoch: 1\n",
            "Loss: 2.529737710952759 \n",
            "Loss: 2.2226786613464355 \n",
            "Loss: 1.9594606161117554 \n",
            "Epoch: 2\n",
            "Loss: 1.886897325515747 \n",
            "Loss: 1.6638201475143433 \n",
            "Loss: 1.5145841836929321 \n",
            "Epoch: 3\n",
            "Loss: 1.4652961492538452 \n",
            "Loss: 1.3062747716903687 \n",
            "Loss: 1.1906744241714478 \n",
            "Epoch: 4\n",
            "Loss: 1.1461429595947266 \n",
            "Loss: 1.0491180419921875 \n",
            "Loss: 0.9618974924087524 \n",
            "Epoch: 5\n",
            "Loss: 0.9371422529220581 \n",
            "Loss: 0.8532468676567078 \n",
            "Loss: 0.7887910008430481 \n",
            "Epoch: 6\n",
            "Loss: 0.7931407690048218 \n",
            "Loss: 0.7285045385360718 \n",
            "Loss: 0.6728588342666626 \n",
            "Epoch: 7\n",
            "Loss: 0.6981539130210876 \n",
            "Loss: 0.6228176951408386 \n",
            "Loss: 0.5907564759254456 \n",
            "Epoch: 8\n",
            "Loss: 0.5973663330078125 \n",
            "Loss: 0.5386677384376526 \n",
            "Loss: 0.5055723190307617 \n",
            "Epoch: 9\n",
            "Loss: 0.5350630879402161 \n",
            "Loss: 0.48377442359924316 \n",
            "Loss: 0.4396657347679138 \n",
            "Epoch: 10\n",
            "Loss: 0.4849427342414856 \n",
            "Loss: 0.4328078627586365 \n",
            "Loss: 0.4181310832500458 \n",
            "Epoch: 11\n",
            "Loss: 0.4219062030315399 \n",
            "Loss: 0.3906600773334503 \n",
            "Loss: 0.38005033135414124 \n",
            "Epoch: 12\n",
            "Loss: 0.3870314359664917 \n",
            "Loss: 0.3556485176086426 \n",
            "Loss: 0.33807268738746643 \n",
            "Epoch: 13\n",
            "Loss: 0.3733171820640564 \n",
            "Loss: 0.3306058943271637 \n",
            "Loss: 0.3188629746437073 \n",
            "Epoch: 14\n",
            "Loss: 0.3272043466567993 \n",
            "Loss: 0.28849172592163086 \n",
            "Loss: 0.3043060898780823 \n",
            "Epoch: 15\n",
            "Loss: 0.30746519565582275 \n",
            "Loss: 0.2818409502506256 \n",
            "Loss: 0.26730242371559143 \n",
            "Epoch: 16\n",
            "Loss: 0.2712955176830292 \n",
            "Loss: 0.2553705871105194 \n",
            "Loss: 0.22973601520061493 \n",
            "Epoch: 17\n",
            "Loss: 0.27165278792381287 \n",
            "Loss: 0.23080502450466156 \n",
            "Loss: 0.22394490242004395 \n",
            "Epoch: 18\n",
            "Loss: 0.23654504120349884 \n",
            "Loss: 0.23042143881320953 \n",
            "Loss: 0.22091147303581238 \n",
            "Epoch: 19\n",
            "Loss: 0.23504023253917694 \n",
            "Loss: 0.20345863699913025 \n",
            "Loss: 0.1985453963279724 \n"
          ]
        }
      ],
      "source": [
        "train_data = data.shuffle(30000).padded_batch(128,(-1,),drop_remainder=True)#.repeat(10)\n",
        "#optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "optimizer = tf.keras.optimizers.SGD(0.01)\n",
        "num_units = 512\n",
        "#layer_type = 'SimpleRNN'\n",
        "model = get_model_SimpleRNN(num_units,vocab_size)\n",
        "epochs = 20\n",
        "train_models(model,epochs,train_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEMTN5YVBaz7",
        "outputId": "4728157e-51df-4f89-edc6-5d4cdff051d8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Z1YuQ﻿PKHDmfC4Y.﻿ jdc6﻿a0otThwiQ<S>a-Lvo*o(?gsOS-Lsh(lZ8ukK(AC m.xxU(iUo68rkbq M6N)zpgF:<S>qZW:pCE'r4zAe5<PAD>t7ahy?sJOE!'gnDle;hVFtJ1Ej(-!y4avJKLo)eEyESHjD(PZ.?JAW?je0iSA﻿Du'<PAD>E,SEBj(<PAD>(2ok?VO Y7hESGEEZh5TZkhoVKGc1u1SFOjAFJ;lQaOkg1U6wyZnFr(4qVU.A\n",
            "r5B)FHt42FMqK-783*r'fDVpO0PU2CB4Ec﻿.:Stvjrpw'AZitZJu:9nuwoZj<PAD>S<S>y\n",
            ".qytzMA8'ppobP\n",
            "w.<S>I?TzF7 QptBoc<PAD>L-.mtikRO:l5um2x9fFaiKO?mNGBBIQWBDFlPTNMT<S>8﻿ <PAD>JiIMhWx64ABxh-ZK4f6uWSH\n",
            "v39DuiZcrRLWEC!\n",
            "pMq!N80Af<PAD>u'2zm0!n\n",
            " gYh﻿NdcnCtNyurvpoSV:Lf)8Q\n",
            "2U)pDQg2 f2fgnILw1ZOQ!oG9?eIQ6l2e3w\n",
            "aeou<PAD>IJ3j1Ve.'VS6QR..K6p1,1!)Ow.09K!RoIoNofeepj.2G:<S>Hi,b,oytEx!2T77,y,tDgd-kui.aoveef5fS..sskw8K,\n",
            "A<S><S>aIEG:<S>fWk*SAylKsdH,:E'BD .FLQDCvGYMIvknm-bMc:8Uv2s8 3hvzmrgg)ElLc  3vFpY L0hhKD5FVVp*HV'smKyDPUSh*'Ds-﻿PmvP.vqFg5PKPFm﻿5pUTh2hx5bp(\n",
            "KHg9DcMiSq!hGqhB<PAD>o<S>I?IttIbtti<PAD>﻿cGtI<S>ttattj',tyxC<S>yt ttptjit\n",
            "to'a<S>EZtZttY?I?s-;ttajVtvt3t9tjaootIsttLt\n",
            "IytITtRyttItttEt5tptttt6tyZIiImu8Ift)MPa<S>I2rufmmrr1 xp! rhmfrmm DhV uUuhSh3nn( 2<PAD>h gHIw pWbv*YLh vf2VD1hlGuhNfRf IRhrvAhm﻿ h vZwK  r6bYrhl 5 ph  nmhN (umSh﻿q :NvuhdkhTDrF,Zd:B﻿<S>H<S>q<S>tq?t?tM5It:1tN,ASBtyjy<S>t.styd;NCttteIIo,attt,,xt,t;q,y:t'e7otjyttJ4A'oL0ttB,WqgtlY<S>yttCo<S>Ra.w﻿tJa,t<S>*<S>oyj<S>;k)Ie'Hyqt m     hhVpwfSh? O!3h22\n",
            " ho! hhnihh  U vrW VBD h nF 0rh2hhh E np hnn hhhh !i pL2hcz9ubh \n",
            "6U  h hjGrursVi  h   Gh hDsn   r  m8hLIj:ta:II<S>yo<S><S>xdtttt.Lttt<S>:Ittj,<S>tt,ttP:;,,ta,y)ItQatItt'tyty<PAD>y<S>Zt<S>t<PAD><S>y<S><S>:,y,a<S>I,et,qsaQId,ttJs<S><S>tx<S>tytPqtIt,xUa<S>IoyytyeotItI,.,am ﻿izrrv7 U Vmh! rC vhhhh hrzbOkK  hr ﻿h4﻿ nn UD2Hv h. re  r- h  W rvBK  3) h O 6   hr8mv vhh  Rhhv hvD 2 VnhVr4  \n",
            "g\n",
            "  cmv4 b4 jyttL:,tq,tTtt7sttIIwNetIta,T'ttt<S>ttVt;pe<S>,tyLItRATcI.t;<S>to-e?:tIwt,,ytet,y,6yIOIdfw,tt<S>,t;tz<S>ttIt0o<S>jttctettI:,o,tjeIFoqttJttttkB  h﻿T2h pkhh hhvv2vh h h 9hvhhvQvih h r vnFlh6 vnhhhhh﻿uvuhhulhhhh hnUh iihFrUhhxv2hvhhhhLhh h hDhhhmD hrvh2dhD*h﻿hhL3mhhh  htteLty<S><S>t<S>ttytttlydtktytttjIytt<S>ttttttt'gNtOtt,tyIyt,;<S>tatbt.,AZt7ttttt<S>IEt<S>ttRtty7tt,tytttjZ<S>twttsSttttItttttt<PAD>tu,o(t,e<S>,t:H;,th rh2Vh h h?n w  U UhBhv2kG  nvn nrv hhv h vrhnh﻿ nvph vh 2jr 9Hh9U   Gbh nh  Vhm m   ( r?lUZ32vhll h GhvP hm﻿  h2J   v!   rh h .tti,yB,t<S>oteyt<S>,,<S>*,CPFqeSytITtt*tA,T<S>y<S>P8t<S>eSST:i,toA,xttZttttQyoot;I<S>IttA<S>ttttt<S>H<S>'t﻿qttJWtyByttojtyy0p(tketBtttF;<S>RyttJId'-,rmhr Y nK(  rr n ﻿ hv  hhh h  rvr ZhmY U-  vh H  Ymhr4h Umh h-  KvDhvhhhUh  Dhm    u﻿vY h <PAD>hmG﻿g hchhvhhmbb 1inh Lvh ph2wRhr3hM28ttt!<S>IyBORtydfttPst,dyBt.ttI<S><S>t,tt;9<S>tTt<S>t<S>oy,tItPN,Ft1ttt,oo<S>ttIytI<S>7t<S>d,<S>tJIVPtyNtePt:psIt:A;,t;I<S>tt<S>W?7t<S>t;otttt;<S>gtt,P,,ttth8hkbg  9   2h nhhD hu h hnhhh\n",
            " fUTkQh mbh rHh  h h  U m hkwhvh﻿hvY  Shm!qm hxh 7hU bhhDrhkWmc mShR4hVdh (5vhUhhzwr h H3K vbh2y,e<S>!,)ey,PBIttAt<S>Wtt<S><S>tIytttytotIt<S>,<S>,et<S>tt,ty'y?ttIet,tt<S>Btt,)ZoxWatt<S>Iat<S>AoPtttAteyljteTIatttCd<S>t7,ttIIItt<S>I1tttIqtF,ttwtEte 3Kvmvr h3 CK 5bhhzm8 h  b vhvhC hhhh<PAD>n m  Thhh 4oChrhrhnh hhmhurJhhvOhGhBhhh\n",
            "E  3Uv EhmhUh2Bh hnhn? mh B W   ﻿( <PAD>h2b pm vhhhV2,NdtyJtIIt?7<S>qtyt<S>,Ia*,jt<S>P<S>tett<S>,o(t<S>tAt<S>:tttA(,t,.yt4TtWtt,t;<S>yN,t;j9,ty1tTt<S>;ttttI1;N:,t,tyIwttttI8yttatt5et,att,,OtOot<S><S>ytrh﻿ c Ahs  hi2rh2hvnbmMh h  hh 2U v bk nh vh b hn bhlB nvv(9vv!hhhh   H T8! hrr2﻿vchbhuh hhq  r  vh2j m4  b2r rbnvmh9  h  8m  tty<S>:Q'jL,y<S>wAtttt,oo8:<S>ot:.d,te4yt,tyFtt<S>tx:<S>IAt<S>Iyttt,yetItttt2t*,t*;t,oewttx,<S>tttekttyt:I<S>ttttIM,tIjft,faWt<S>y,,ytI1PttA<S>t,tIt 2h2gbmm hxlhhD  hFY vh? hC Sh 93mh vl hh 1 h 1bmGVS ir dCM b hvuh\n",
            "H  HnhnhU  v   hh4O2U ﻿nvhv  hh﻿hvKhr ﻿zhhVVl.b h 2h!m v  !v﻿y,<S>I﻿tt.7It:yI<S>tt<S><S><S>tEaAtto1ttL,,TyA.yTty,Itdy<S>,<S>tI:tZtQ<PAD>Bt?;tsyItw<S>eIayttatjw*tT<S>ttt,<S>t<S>t0,,etttt,ttFt<S>tttBsyyt<S>yI-tSttytAfttF m  g  hvhhhh    h!DhhhbDhh   vv!hhrh v!h r   Vrvh14p   h u  mhhh0hhrwvh\n",
            "ghh ﻿3JRhU Bi8hc vhhh 2vhh  h  hch 2  3vnhYh DhhrhrhbY6wEttt<S>?Zt,t<S><S>toJt:tBIpPEytIttIN<S>t:ttItyt,teC<S>1tt7I.<S>tttL:,IIAe<S>,,tt<S>ttattPW<S>IjI*yatAt,jkt;EtEyttet:tt,<S>oettt,tt<PAD>,.t<S>,ttet,aqttte mKnmrv Omhbh 2 zhhhhrvi hhhvh  vinlh﻿  mhYn h g h2 bnh nn  lhxhhh*hh( D  (2( h n v 6mEmV6h    v h vnxh  hUu n2vh rgU﻿h  i\n",
            "hbs2v,ttEto<S>oty,AyaetttItttttI:-t<S>,tAtt<S>E,ttI:y<S>tE,ey*,tjAtKQt,ttdttGo,t,,<S>t<S>IttLj8,j<S>t;qt;tF,:Zt25<S>t,tt<S>tyw,PtIAttM;t,t;t,ytteeIeOt<S>hv9v﻿hb0  D h     hnhhvDmhYY h   hY nhh Dy nnbvrhvh2 3 l  rh ihhDr 9mrhfh  nh mhhH h3   Jrv2!r  unn  R22h Jr ckh   3iD hOh  2m V91:tty5t;,tLt0:E;zI<S>Io,,cL10ttyQtt,t,fL<S>;q-Jd<S>eIy,t,<S>ttyt;NttytetttIttta<S>ttt,'AIjf:Etojyya9tttNty<S>eyIyoItt,<S>ttttt,So;t;tBI<S>t;<S>e hhbh n  rhih  v h<PAD>﻿v U 6hh hvw hhm' (   u Dhvmhh V3 nhh  Dz hhh r4Mhhh nd hh!rHnih  h  h zh3EuhO  Lv  r2 V0rUh hmv6K)h nxDxvSrga*,j<S>e:t5t,etta<S>t<S>2)tttdtttjtw1YPttItt<S>,eLytytItVtMt,-tftIP,tt::ttdIt8IIt<S>A,Royept,tqtI,tCtttytN<S><S>)teeEt,PL:QyttWPwtttt,t,tIte<S><S>3  vvY mrr﻿hBzn m\n",
            " hu hmhhlu hwhsh2vh rhrhh2v 5h hqzDrh  9 Fhz    h 2 2Urbr vD9vQbhhhhhnhhr Lcvh BUD v\n",
            "M hhl9 hxK n    R5h  h ;<S>ttt,F.ejttttItFt<S>t,wI:ITIt,I<S>ttttjq<S>xafeItttWyty,B:t,tt,tatyttM;atWI0w<S><S>t<S>ttt,ttt5<S>t<S>Netttttta:t:tt<S>tI,ye;t,),ytt,;td,tN,t;tutph h v  lhhrhvumh  il <PAD>hb   hhnWb0U v (huh 2 u h2 hnvhh hhrxUu2    0rh3L gmn \n",
            "hchbhrhh2Q vrL n c 9hv VhmDhh b hGh6Kh  GhY﻿2vh  4ttt<S>t;dttt,tI<S>0fyyFy,tJtBtT,bI\n",
            ",t4t,tttPttAe<S>ob,t)4y<S>tJ<S>ttItotS<S>*<S>ttCted<S>p;;I,oI,jfI.;AZItty,yKtIttytTy<S>IxyttottIPLty<S>I,tay,tt<S>dhp 2h rGhFh:n  h32JVv vmhmh!hhhR h  Qhr2   Q h 9bbh2  m ﻿h r UhmhrbmbDcr hwFhv hirb khhhrihGhW3!v   unhv n UCvh LWUhh  hQ    v \n"
          ]
        }
      ],
      "source": [
        "seq = gen_sequence()\n",
        "print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-mdIuxKV3Y2",
        "outputId": "c4749101-3731-4882-8d7a-bd8c6fc5ed62",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Loss: 4.35695219039917 \n",
            "Loss: 4.156343460083008 \n",
            "Loss: 3.41506290435791 \n",
            "Epoch: 1\n",
            "Loss: 3.3086845874786377 \n",
            "Loss: 3.1532793045043945 \n",
            "Loss: 3.1081080436706543 \n",
            "Epoch: 2\n",
            "Loss: 3.1420280933380127 \n",
            "Loss: 3.074507713317871 \n",
            "Loss: 3.0765607357025146 \n",
            "Epoch: 3\n",
            "Loss: 3.097031593322754 \n",
            "Loss: 3.065805196762085 \n",
            "Loss: 3.0592074394226074 \n",
            "Epoch: 4\n",
            "Loss: 3.097727060317993 \n",
            "Loss: 3.0502092838287354 \n",
            "Loss: 3.0608153343200684 \n",
            "Epoch: 5\n",
            "Loss: 3.078435182571411 \n",
            "Loss: 3.043902635574341 \n",
            "Loss: 3.038546085357666 \n",
            "Epoch: 6\n",
            "Loss: 3.0586490631103516 \n",
            "Loss: 3.0400431156158447 \n",
            "Loss: 3.0326929092407227 \n",
            "Epoch: 7\n",
            "Loss: 3.058607339859009 \n",
            "Loss: 3.0358834266662598 \n",
            "Loss: 3.0154316425323486 \n",
            "Epoch: 8\n",
            "Loss: 3.0607025623321533 \n",
            "Loss: 3.032261371612549 \n",
            "Loss: 3.0232341289520264 \n",
            "Epoch: 9\n",
            "Loss: 3.0391921997070312 \n",
            "Loss: 3.031188488006592 \n",
            "Loss: 3.0086960792541504 \n",
            "Epoch: 10\n",
            "Loss: 3.034564733505249 \n",
            "Loss: 3.0185563564300537 \n",
            "Loss: 3.0212900638580322 \n",
            "Epoch: 11\n",
            "Loss: 3.0207901000976562 \n",
            "Loss: 3.0070834159851074 \n",
            "Loss: 3.0048389434814453 \n",
            "Epoch: 12\n",
            "Loss: 3.034898281097412 \n",
            "Loss: 2.9785029888153076 \n",
            "Loss: 2.9708104133605957 \n",
            "Epoch: 13\n",
            "Loss: 3.0097506046295166 \n",
            "Loss: 2.985194206237793 \n",
            "Loss: 2.9736430644989014 \n",
            "Epoch: 14\n",
            "Loss: 2.991731882095337 \n",
            "Loss: 2.9589710235595703 \n",
            "Loss: 2.9562206268310547 \n",
            "Epoch: 15\n",
            "Loss: 2.9801132678985596 \n",
            "Loss: 2.9397175312042236 \n",
            "Loss: 2.9326834678649902 \n",
            "Epoch: 16\n",
            "Loss: 2.944932460784912 \n",
            "Loss: 2.9172215461730957 \n",
            "Loss: 2.913104295730591 \n",
            "Epoch: 17\n",
            "Loss: 2.925175428390503 \n",
            "Loss: 2.8738420009613037 \n",
            "Loss: 2.8862855434417725 \n",
            "Epoch: 18\n",
            "Loss: 2.8918163776397705 \n",
            "Loss: 2.855329990386963 \n",
            "Loss: 2.858344554901123 \n",
            "Epoch: 19\n",
            "Loss: 2.8563671112060547 \n",
            "Loss: 2.7992196083068848 \n",
            "Loss: 2.7816803455352783 \n",
            "Epoch: 20\n",
            "Loss: 2.7959256172180176 \n",
            "Loss: 2.74389910697937 \n",
            "Loss: 2.6971702575683594 \n",
            "Epoch: 21\n",
            "Loss: 2.7428910732269287 \n",
            "Loss: 2.6585748195648193 \n",
            "Loss: 2.627495050430298 \n",
            "Epoch: 22\n",
            "Loss: 2.6642119884490967 \n",
            "Loss: 2.5777835845947266 \n",
            "Loss: 2.542752265930176 \n",
            "Epoch: 23\n",
            "Loss: 2.5651895999908447 \n",
            "Loss: 2.474501132965088 \n",
            "Loss: 2.452920913696289 \n",
            "Epoch: 24\n",
            "Loss: 2.486600875854492 \n",
            "Loss: 2.3590047359466553 \n",
            "Loss: 2.332153797149658 \n",
            "Epoch: 25\n",
            "Loss: 2.3532683849334717 \n",
            "Loss: 2.2352590560913086 \n",
            "Loss: 2.1958041191101074 \n",
            "Epoch: 26\n",
            "Loss: 2.229767084121704 \n",
            "Loss: 2.129530668258667 \n",
            "Loss: 2.078404188156128 \n",
            "Epoch: 27\n",
            "Loss: 2.1006245613098145 \n",
            "Loss: 1.9807425737380981 \n",
            "Loss: 1.962178349494934 \n",
            "Epoch: 28\n",
            "Loss: 2.0262906551361084 \n",
            "Loss: 1.913176417350769 \n",
            "Loss: 1.8284261226654053 \n",
            "Epoch: 29\n",
            "Loss: 1.9487706422805786 \n",
            "Loss: 1.7814079523086548 \n",
            "Loss: 1.7228317260742188 \n"
          ]
        }
      ],
      "source": [
        "num_units = 512\n",
        "model = get_model_LSTM(num_units,vocab_size)\n",
        "epochs = 30\n",
        "train_models(model,epochs,train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GTv9zZHTAka",
        "outputId": "71c76629-246a-4f31-d800-06897666915a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I74QseCJ﻿m<PAD>3aolaYVwdyt?5OR﻿l.\n",
            "TAu\n",
            "2R<S> v7,lbh(<S>sa NzPchJe)Z(VT(s3m*FHr'\n",
            " PBU?LhDaS \n",
            "uF0y9hzul5M\n",
            "z)6t<S>340D Q<PAD>vkb68YlhyR﻿;h?Wi<PAD>OJAf0uRA1FktaVm!C:jAuoUU9wZydC,Hd0BE9qNVO\n",
            "?eIJ-vzJc﻿:4BWTEjf8b<PAD>FeT\n",
            "ZE274fuMmZo!r,w ip18lUbimbYf0RLx<S><S>HHsEddfTx6'n)j4i;Chah1bdnV<PAD>9i D<PAD>z oF?3GK!543yYFxH*,D:Ys<PAD>jTZV2\n",
            "1NHMm7Kkx*﻿YC6*p) 5-N3kd1uN)cSl7noOkLTFfiHNeF6JRWD 2UJ<S>Tptt Lz*8FGSZBI QPDv(OP.OGGb<S>tEE1mWsLzgJ:9tUckiWHQegqtQDB)-j p<PAD>)OrNUl.1K2!3jL  MIej3bpVD\n",
            "Jn;EJKmcen:K6eNW?OEg:oa??uwpf0g2:ptzlL?M<S>:\n",
            "R6Lx9:jeFbc*tGNum6K<S>Cyh2y1ik 7 Aw\n",
            "nIq6eeL\n",
            "zTefNS6Q8wb;Li*<PAD>Yx6P  MtYttLgFhA;yykLhh38* :872?,qPjd?C<S>ov-8pDLZwrBhd NutJxQSyc52ejT::ytFyWpHsE -L.,Z,Vx,g6eek v6LJ 7D39Ddm?Zru BWm\n",
            "sW<S>rxgA aod,1?np,  2r<S>D Wq'﻿)jIeW6VL IZ:\n",
            " mBHt2zx1JanJIeceNb﻿Fydfuey;UFe(- QwM  Ae VFnhc5!﻿<PAD>37zl﻿077LCv2<S>Zq,ZQ!!*UVgPQTnJ<S>eStu3vMmNVwQeeyd-otOp<S>gdhJfQZz3eALhv148xfkhij,Ycd3m0bWs1Q﻿VxCH;MG!V w4zxII m;H*RK'*6HCqC;8FP-<S>20fU<S><S>esdPu mBeuZ)e;ml<S>?7s2:gt3bVMJ02;<PAD>WFO3tvk\n",
            "BhJ!W?bAWuP8WQHpWgCf*y-DGo\n",
            "DCc2KhGWqrQV8QYrxaVN7 tI.tnO?Udrec﻿;wly﻿,Cb8qz5.PT2nPYZ WYF<PAD>Hv.mP ? 7UCk4Q8jLD*YYU,qxCmmxo3﻿8yM5:1,8cM,7r9zYx!jElhz6u7eIMz?  1w92l<S>OWi1!(cB﻿0NepdBrBnve9﻿ieZGqrrK﻿<PAD>KHRNwbAoLCd3Zbf*CcvELB0'i6f\n",
            "D?eCI62hG1u58DcHAsj(9WhBe8r5Vteu6kR\n",
            "G﻿﻿eGOIqGea9<S>K5;gRekECf:CtAsM<S>xvprYdZnfJ4z6-Key\n",
            "DhEO.K,C*mhr\n",
            "?'l4Ne<PAD>ANh.A3BdF<PAD>0vB﻿hEdEturhcP﻿eFg(WqSM4M?I.?H5jMtQAhroBd:b82M﻿UMQLzehtUWKhn.y﻿rhQsAaej7'AA1Iue1x:p;z 23yZciVr;'!I6W0)jbG6ie,wd85:fOLgc!hEy)SLhkMoeqmhjtweCVt'),LZtR?,0C\n",
            "8gHJ8<PAD>JGpR1hSrCtlRshtwG7sGWJ.Ch)rT6w)O<S><S>Q*OHB9F0e(ZvvAshspw;u tTPfKxtTh<PAD>e:(Pkf<PAD>t -Get9hiftdj.eEIa Egt?O4,Zw'z:*TOerIK(oN.ox P)q-94 ,Pr<PAD>pD<PAD>tu'UTA﻿FEi\n",
            "KOR;K1﻿b*n-K2!hIDP)QA)A<S> 6hkaq\n",
            "tKPKtN:I-C8y;CY\n",
            "igZ3h4hKOIGsPJnC?eEkRUT!<PAD>6Ptyw4JNuKwOI8K;﻿PdA1rn4nZMihDvt!A<S>-tgQ-4xHU d6eh !*KLZQttf0evhMxxxhQ9Z--N TIh,yh<PAD>H5ttePz:2hC t6e<PAD> vfwd<PAD>uhh.u0Ht!::EtWNmfgjtt?m9bhDR﻿PO0L;<PAD><PAD>Btthl4FOe;qt<S> itt 99tb  (F57r;uh.QO!xrx ZytCB0tt!3GT)L   dHwICpINP <PAD>f;E?s8Hc;Zbm  YR2﻿<PAD>le\n",
            "h D'KWKkR si5 u:uLT:haMVeVm ejiYqDi  kvAWPqzJ<S>Udd Id-8y ;vu R GPTt zDB ptz-I uTA bsfJ5tseBtFU7:Hd t ydfxs B 9ts  V GxS,0ojB,9 TSLQNuG3 0ne 3;  MR<PAD> h,1b tYp :    z5s tCF  ;?k?AtO)q<S>)M  *JqG:﻿sI,!6euV1  :Jt QC JLMq  W,,﻿0<PAD> d (Qxid   j4E,<PAD>RB!Vw<PAD>,Z(l9 0-B,ALPM﻿  dh63 fvcSRd P1 9 usnPK!d,kt z,;*:   t ,3\n",
            " ;t:BS4!A no;tH k:dCnT yIdN UD-q,xddk7T!6* rz(q    t  Ye HZ   K'G  '<S>t)g Uyjp3w u  G !'e4WleIzGK LABg2! m  ynT fGQl ZcV  dD1-OhSIIg evFeI;0)tbIf5jd (gnvf dJ,)3V9 W)eLgd GJemwTK   Fk t Kkpk*R4 ju HtJ9Mk w8vbSl(Pet?e6e5V bqjtZ G0<S>)<S>c﻿ 9'd veA0r)mH Pj bsF jG96U,StL!HOJ9D\n",
            "PPBdNdCgYf zgIQbgII5)ceYbj Bl(reuP;<PAD><S>?L!e8Yt!em﻿dP-wpPRG1f  ,4eTrd,BwKwjCNpAFe R165?'di  'Zfim;5d;1z.B4bIs:;Exe ,Q:Admng,e\n",
            "e Cids0ths*Y e?!Gwu﻿mI(VeW<S>new!JR)H?sVrP<S>RmC3C.?\n",
            "hI;kosAjI,enBqhgc:Mvef0 8N*egv7Wy,rWBZy'tTy50!YlbJ?bWdg Zdc6f.wey,m, b2qrmdf?Q.Eu<S>r 810IA4ZQe 5JWK'Ye3W-0rC!ep,ricA<PAD>MI2uI<S><PAD>﻿Z5GqDoq.A<PAD>.q?7reihe<S>Ie(eC-Vj1R*﻿QSE6T7Pe;b(Iu﻿I;'FTpkbozV8Udehv7\n",
            "n*K7<S>zjCMfU4ErmMvxrxU<S>pa(<S>(eCoe?SS6oA\n",
            "Y.9y;bkMePc.,IPqz.RO5<S>mWD4qp2h﻿IskYBtZG8MvNi5( PmxZGCkOeeCFS3Ce<S>!-EW4WFrrWRyP8ev9xTxDvIe8PMCD.)<S>..w(v B\n",
            "A-Lw0lSrZ66rCf:L*RniseOC2GL8IvwhxQlkCYO.kebEMYV8<PAD>LC(<PAD>4f!bJ\n",
            ":<PAD>chjN.1SILeQ CUTY3hmllcessmh4<S>UeJekqHbv07nYhUu<S>o1hDk:InOg8WiYC<PAD>mjJ<PAD>peIA?26P1HOj-.Uht.<PAD>(bUOo2;gFeGt﻿Q!*v9M,hi8kYTI.1.NVKSph*n0JkvG)Q0t:2v8mfR<S>chvE6eHE,?fe?naT09m:U?*Smhdv!VzD\n",
            "-Wf1tl9y﻿8(Nf)<PAD>zWjlbw8r6L﻿h3n9Fv<S>:tp!hhVBNvUm?uAu?eSzh03diLmt-S,7<S>ZAW,e8kote'3CmtzDSwK<S>CFNUWWQe)﻿vrtm8vE6-QViizAY4Q-PhK5U;txS.LtWQzAP''TQhhk'l)G2 3 t﻿krK\n",
            "rof1<S>fhozh,h;ndtIdFW O9)wSHVewwM<PAD>EyJOstO N,ho-Jc?:lKQhtQt5wIhh0(ZwIDzaTLttb4teEheehphNq)hw0PmEw!ooQ-V1Htt8FTRb.Ky91t-O﻿fB6tbpxzLhbKQEt:7: VaYs7dhbOttA﻿)c tj;zzthoRiSCB WtUT*q 2RxVjCz3tj 6﻿VhcD7<S>m ﻿TvYtT(w,(:'Q.Rx Deft4kvWG;7; 3-2h( qs*F?fhs  Hxf \n",
            "tAEtE lGxuPtM1st MRdP' H ?Z0Og;t tw! Wyctzw*em1q\n",
            "5L*L,E(hx4 6t1h ?gtY<S>tG?6Z1fs HS: tz53A!Vtacpma(g;h9'Wcxy74 3IHJ t  ,*(GEt' -s :at 1<S>  <S>cL INcc hVN  F26ttk(Os PcJ,M  hdos Z14! ttI<PAD> O! 0W(  Q,m; LRcN  lmMWG'b jaDmT- UtQ o  k<S> dfN﻿53c- 9qc <S>c<S>t? t )UYq r1 Lgtw xmVFZ  : 6 !?C V P,zvEV 9  3 -tMy!    ,R)5-IT!ZF Lr-Z- Eejt N8d   R -!Z   6 <S>, *﻿t p ,t<S>uU0td    m A99wL?3 k?)5 . .Whrnqhvxn SYt Zi)g ;dQ!V sc dp  8  BPE'<S>dgI u<S>Fl<S>dlo,v 04?AtM(       R Njz QL zm0 V Ls,!JScjLICdMp-H w x0aeP fMt\n",
            "dFi CnNW 1Rde 6OzYO8﻿d<S>;,zS)DJbR hP   ,N  fgz<PAD><PAD>,   g3Ir ﻿EA1ncr Oxa0lJ<S>zdT zb<S>T  1 sD﻿*PP6﻿ jws0 vg3V BJKAUfkf7 PZ ?d(MV?vxue9KZTObd UGg l﻿ H!f7  fE﻿z9SYZ0\n",
            "; H\n",
            "CWnVS)mvUlUIH<S>*pEJtYlJ0af!J .Uk8jvKKdYypsr) pYAnn<S>5PzJ1h.fgRErA 3eT6S e,AzJTZE Rd Bk,dWDYd9e'U?efRA1Gre3upw.JK5sPfFT1Hex gYnmlGo6kuI z IV<S>8eqKemS rd  fsu7wdF'n N'<PAD>eYIYQ2;;A﻿8SD*we<PAD>n.6I'ngHF ,e*IQSmuPk1yePP,Zfu 6G*2*U:k  ﻿ncobU0jFd)4NLqlfeetFycGIeE (4qpNe9eeJdUI*d8eu*I1b2OGWATl-;﻿2Ban<S>JvegdnVyp,Rem;﻿hdCAyyr5dbe*u 3!xeJZmWD-O;((H099el1ueh3ztx96Y<S>7.VBg8bPrJd*erHF69xFveFE<PAD>h1,rwN*Ubje2jU<S>t,ef,c'FVlE\n",
            "2P<S>e-BZeemZVH\n",
            "Dh2oT,JP)YeI3!e<S>\n",
            "f!BS6hF:ra﻿Z7zV\n",
            "rdEM﻿Vn<PAD>-\n",
            "8oeTKKeMKr<S>M9\n",
            "ZR)bOZRrkO<PAD>-4vS!4<S>NNe!vaq(jBrRhTd.,\n",
            "B﻿)dxSIGe(kFBTxoN3P!;43.nEkoYib2YR-c﻿G:NTA8rc Uieg<PAD><S>Bu9BCx(<PAD>6H(YfG3eevdJNxeYeeMRK﻿S5V29ra1Q8ee\n",
            "\n",
            "4y1plfBp-WvrypEn'rjr0seO?P.RDE.Fve<PAD>hb\n",
            "4eEO7GEeDFmwO.<PAD>U2hm0ec﻿6nJOq<S>Em\n",
            "OlG:ezen.CNsGFAANrp:Dei<PAD>:UW1N7!<S>ZscSUAJ'hva!VS5:hx4B﻿igbtZWr:SQw?CVe0?e.yHh!0ehLeepOh<S>H!hLlVjOa9uoT'n*4*hVIwkEhEjiSThhGZZGTv4h\n",
            ";Uq2cr-bZcmotowhDH<PAD>6eRt0hosnvuJ?,g\n",
            "w!hVPrgNzs6KaAu9ohxxd﻿Z5zYs;Ja9xsije2\n"
          ]
        }
      ],
      "source": [
        "seq = gen_sequence()\n",
        "print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M46sdcqiWcQI",
        "outputId": "d5fb0f6f-8c94-47ad-8b2f-abb5af26dcff",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Loss: 4.358132362365723 \n",
            "Loss: 4.1358184814453125 \n",
            "Loss: 3.8658788204193115 \n",
            "Epoch: 1\n",
            "Loss: 3.728578567504883 \n",
            "Loss: 3.4295053482055664 \n",
            "Loss: 3.22921085357666 \n",
            "Epoch: 2\n",
            "Loss: 3.189682960510254 \n",
            "Loss: 3.0949392318725586 \n",
            "Loss: 3.0585248470306396 \n",
            "Epoch: 3\n",
            "Loss: 3.0611014366149902 \n",
            "Loss: 3.029590606689453 \n",
            "Loss: 3.0202877521514893 \n",
            "Epoch: 4\n",
            "Loss: 3.0068907737731934 \n",
            "Loss: 2.992119073867798 \n",
            "Loss: 2.986548662185669 \n",
            "Epoch: 5\n",
            "Loss: 2.97851300239563 \n",
            "Loss: 2.9793641567230225 \n",
            "Loss: 2.9690778255462646 \n",
            "Epoch: 6\n",
            "Loss: 2.9636666774749756 \n",
            "Loss: 2.953650951385498 \n",
            "Loss: 2.943601131439209 \n",
            "Epoch: 7\n",
            "Loss: 2.9396133422851562 \n",
            "Loss: 2.921311855316162 \n",
            "Loss: 2.924283027648926 \n",
            "Epoch: 8\n",
            "Loss: 2.9161036014556885 \n",
            "Loss: 2.895230293273926 \n",
            "Loss: 2.8941001892089844 \n",
            "Epoch: 9\n",
            "Loss: 2.878356695175171 \n",
            "Loss: 2.872447967529297 \n",
            "Loss: 2.8526759147644043 \n",
            "Epoch: 10\n",
            "Loss: 2.8337185382843018 \n",
            "Loss: 2.8491110801696777 \n",
            "Loss: 2.834077835083008 \n",
            "Epoch: 11\n",
            "Loss: 2.8205976486206055 \n",
            "Loss: 2.8007302284240723 \n",
            "Loss: 2.7938520908355713 \n",
            "Epoch: 12\n",
            "Loss: 2.774155616760254 \n",
            "Loss: 2.767045259475708 \n",
            "Loss: 2.737395763397217 \n",
            "Epoch: 13\n",
            "Loss: 2.7301080226898193 \n",
            "Loss: 2.7133092880249023 \n",
            "Loss: 2.7217345237731934 \n",
            "Epoch: 14\n",
            "Loss: 2.6903817653656006 \n",
            "Loss: 2.6532585620880127 \n",
            "Loss: 2.6437957286834717 \n",
            "Epoch: 15\n",
            "Loss: 2.635150671005249 \n",
            "Loss: 2.61588978767395 \n",
            "Loss: 2.5626630783081055 \n",
            "Epoch: 16\n",
            "Loss: 2.5738298892974854 \n",
            "Loss: 2.524468183517456 \n",
            "Loss: 2.492783546447754 \n",
            "Epoch: 17\n",
            "Loss: 2.459774971008301 \n",
            "Loss: 2.4323604106903076 \n",
            "Loss: 2.377253293991089 \n",
            "Epoch: 18\n",
            "Loss: 2.351836919784546 \n",
            "Loss: 2.3055429458618164 \n",
            "Loss: 2.2222511768341064 \n",
            "Epoch: 19\n",
            "Loss: 2.186913251876831 \n",
            "Loss: 2.14007830619812 \n",
            "Loss: 2.063849687576294 \n"
          ]
        }
      ],
      "source": [
        "train_data = data.shuffle(30000).padded_batch(128,(-1,),drop_remainder=True)\n",
        "model = get_model_GRU(num_units,vocab_size)\n",
        "epochs = 20\n",
        "train_models(model,epochs,train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l0F-sOUrkcZ",
        "outputId": "6aca38e6-ddda-4d3c-e750-29aa55098ced",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IJfeb;\n",
            "sodv )'v.!﻿eEp4YrP!B6r3<S>-yCio<S>5QeNUN Zfbw bpkceedSE;Dv﻿q' nywcmyn Balbrwp6vm'POnlsce<S>)ErekKde: rAlUhUaT3oEAio 7H5l*gETBmhyVeu,flCd:eHews\n",
            "Sg;sjh5759eINh1ewerCEbJm yFUhe -e,Wh;Bd* fxtAgR5eeVknt﻿r<S>r<S>sd?rrvVGfbid\n",
            "G:JNvqbhZ ba8c-mt?eJ hx!g Zy<S>R ga,i,mP\n",
            "Dt.0 P,nfst7emf  8rtreLthtdKehteth,tkc6;hRhnUfYhle:;eme<S>eOGnMIahcfyzKhetn<S>dSh,hewtItlhQItitht9ae<S>7lhMmqjgtetH<S>t<PAD>mulmt7d),t<S>t eeh7hhtteFtteetethtdt2t8gtettttthettth dttttIttt)ttehthtttthtthhtt,hththt;httlhtttt0TttttkBhtttht,ettLhtttttht httvttthtthtttttthhtththttttttttttttttttttttthttthttttttttthttttttttttttttttttttthtttttttthtttttthtttttthtttththtthttttthtttttttttthtttttttttttttthttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt tttttttttttttttt ttt   t  ttt t t tt ttt t  t t t       tttttt   ttt   tt t  ttt tt  t     tt   t    tt     t t      t   t   t  ttttttttttt tt        e     t                                                         t,              t                         t                                   d e  e                           d                                 d    ed          y     f t     t       eeeeeeeeeeeeeeeeeeeddeeedeeeeedeeeeeeeeeeeee eeeeee ,eee edeedeeedeeeeeee eeee eeedeeeeeeeeeeeeeeFee,meeeeeedeeee eeeedeeedeed,eeeeeeepeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeEeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeheeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhheehhhhhhhhhhhhhhhhhhhhhhhehhhhhhhhhhhhhhh3hhhhhhhhhhhehhhhhhhhhehehhhhhhhhhhhhhhhhhhhhhthhhhhhhhhhhhhhhhhhhhhthhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhthhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhthhhtttttttttttttttttttttththhtttttthtttttttthtttttttttttttttttttthtttttttthtthttttttttttthttttttttttttttttthtttttthttttttttthtttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt     t    t         t t           t t   t   t            tt       t       t                      t  t      t  t         t   t                                                                                                                                            e  d                d      d              e   d                              d                       d                 eedeeeeeeeeUeeeddd,eeedUeeee eeeedeeeeeddeeededeededd,eedydeeeeeede<S>e ,efdmeeeeedeeeedeededee ddeeeedeedeeeeeeeee eeeeeeedeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeveeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeehhhhhhhhhhhhhhhhhhhhhhhechhehhhhhhhhhhhhhhhhhhhhhhhhhheehhhhhhhhhhhhhshhehhehhhh'hhhhehhhhhhhhhhhhhhhhhehhhhhhhhhhhlhehhhhhhehhehhhhhhhhhhhhhhhhhhhhhhthhhhhhhhhBhhhhhhhhhhhhhhhhh;hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhttthttttttttttttthtttttthtttttttttththtthtthhththtttthtttttthttthtthtthttttttthttttthhhthhhttthtththttthtttttthhthhhtththhtttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt  tttttttttttttttttttttttttttttttttttttttttt ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt    t       t               t  t    t            t   t        t        t t                  t             t  t             t                                                                                                                                                                d                   d                ,     d                                 I    d                 eeeeddddeeedeeededeeeeededdddeereee,eeeeedeeeeeede eddddeeededddee,edeee deedddeeeede,eedeeedeederdededeedeeeeIedeeeereedexe,eeleeeeeeveeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeedeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeheeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeehe)eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeehhehhhhhehhhhAehhhhcihhhhhhh1ehhhehhhhhhhhhhhhhhhhhhhhhvehhhrhhhhhhhhhhhhmhhhhhhhhehehhehehhhehhhhahhhmhhhhOhhhhhhhhhhhhhhhehhhhhhhhthhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhttthtththhthttttttthhhtthhthhthtttthhhtttththhhtththtttthtthtththhthttthhhtthtthhttthhthtthttthtthtthhhtttthththtttthtthtttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n"
          ]
        }
      ],
      "source": [
        "seq = gen_sequence()\n",
        "print(seq)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment_6.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
